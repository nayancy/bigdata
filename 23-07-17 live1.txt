hive-------------

insert overrite directory 'output path of hdfs' select


mysql -h 101.53.130.146 -u guptanayancy -p --local-infile

wc -l transactions_practice.csv------

-- select database
use training

-- create table
CREATE TABLE transactions(id varchar(20),chain varchar(20), dept varchar(20),category varchar(20), company varchar(20), 
brand varchar(20), date1 varchar(10), productsize int, productmeasure varchar(10), purchasequantity int, purchaseamount FLOAT);

-- load data
LOAD DATA local INFILE '/home/guptanayancy/transactions_practice.csv' INTO TABLE transactions
FIELDS TERMINATED BY ',' ENCLOSED BY '"'
LINES TERMINATED BY '\n';


----------for importing the data into hdfs from mysql
sqoop import --connect jdbc:mysql://101.53.130.146/guptanayancy --username guptanayancy -P --table transactions -m 1 --target-dir transactions_data


vwOT67OkP13Yocm


guptanayancy@e2e-27-153:~$ sqoop import --connect jdbc:mysql://101.53.130.146/guptanayancy --username guptanayancy -P --table transactions -m 1 --target-dir transactions_data
Warning: /opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
17/07/23 14:54:30 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.11.1
Enter password:
17/07/23 14:54:38 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
17/07/23 14:54:38 INFO tool.CodeGenTool: Beginning code generation
Sun Jul 23 14:54:38 IST 2017 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
17/07/23 14:54:38 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `transactions` AS t LIMIT 1
17/07/23 14:54:38 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `transactions` AS t LIMIT 1
17/07/23 14:54:38 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce
Note: /tmp/sqoop-guptanayancy/compile/5a3f58da155cb6a32de38592526396e5/transactions.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
17/07/23 14:54:40 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-guptanayancy/compile/5a3f58da155cb6a32de38592526396e5/transactions.jar
17/07/23 14:54:40 WARN manager.MySQLManager: It looks like you are importing from mysql.
17/07/23 14:54:40 WARN manager.MySQLManager: This transfer can be faster! Use the --direct
17/07/23 14:54:40 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.
17/07/23 14:54:40 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)
17/07/23 14:54:40 INFO mapreduce.ImportJobBase: Beginning import of transactions
17/07/23 14:54:40 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
17/07/23 14:54:40 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
17/07/23 14:54:40 INFO client.RMProxy: Connecting to ResourceManager at e2e-27-137.e2enetworks.net.in/101.53.130.137:8032
Sun Jul 23 14:54:43 IST 2017 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
17/07/23 14:54:43 INFO db.DBInputFormat: Using read commited transaction isolation
17/07/23 14:54:43 INFO mapreduce.JobSubmitter: number of splits:1
17/07/23 14:54:43 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1500551408671_2058
17/07/23 14:54:43 INFO impl.YarnClientImpl: Submitted application application_1500551408671_2058
17/07/23 14:54:43 INFO mapreduce.Job: The url to track the job: http://e2e-27-137.e2enetworks.net.in:19099/proxy/application_1500551408671_2058/
17/07/23 14:54:43 INFO mapreduce.Job: Running job: job_1500551408671_2058
17/07/23 14:54:50 INFO mapreduce.Job: Job job_1500551408671_2058 running in uber mode : false
17/07/23 14:54:50 INFO mapreduce.Job:  map 0% reduce 0%
17/07/23 14:55:01 INFO mapreduce.Job:  map 100% reduce 0%
17/07/23 14:55:01 INFO mapreduce.Job: Job job_1500551408671_2058 completed successfully
17/07/23 14:55:01 INFO mapreduce.Job: Counters: 32
        File System Counters
                FILE: Number of bytes read=0
                FILE: Number of bytes written=153516
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=87
                HDFS: Number of bytes written=60317773
                HDFS: Number of read operations=4
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=2
        Job Counters
                Launched map tasks=1
                Other local map tasks=1
                Total time spent by all maps in occupied slots (ms)=33164
                Total time spent by all reduces in occupied slots (ms)=0
                Total time spent by all map tasks (ms)=8291
                Total vcore-milliseconds taken by all map tasks=8291
                Total megabyte-milliseconds taken by all map tasks=16979968
        Map-Reduce Framework
                Map input records=1048575
                Map output records=1048575
                Input split bytes=87
                Spilled Records=0
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=123
                CPU time spent (ms)=7190
                Physical memory (bytes) snapshot=461705216
                Virtual memory (bytes) snapshot=1626628096
                Total committed heap usage (bytes)=683147264
                Peak Map Physical memory (bytes)=461705216
                Peak Map Virtual memory (bytes)=1626628096
        File Input Format Counters
                Bytes Read=0
        File Output Format Counters
                Bytes Written=60317773
17/07/23 14:55:01 INFO mapreduce.ImportJobBase: Transferred 57.5235 MB in 20.2674 seconds (2.8382 MB/sec)
17/07/23 14:55:01 INFO mapreduce.ImportJobBase: Retrieved 1048575 records.

-------
guptanayancy@e2e-27-153:~$ sqoop import --connect jdbc:mysql://101.53.130.146/guptanayancy --username guptanayancy -P --table transactions  --target-dir transactions_data
Warning: /opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
17/07/23 15:12:03 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.11.1
Enter password:
17/07/23 15:12:24 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
17/07/23 15:12:24 INFO tool.CodeGenTool: Beginning code generation
Sun Jul 23 15:12:25 IST 2017 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
17/07/23 15:12:25 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `transactions` AS t LIMIT 1
17/07/23 15:12:25 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `transactions` AS t LIMIT 1
17/07/23 15:12:25 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce
Note: /tmp/sqoop-guptanayancy/compile/3326ea15c33e23a2f75762e6629ea8d6/transactions.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
17/07/23 15:12:26 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-guptanayancy/compile/3326ea15c33e23a2f75762e6629ea8d6/transactions.jar
17/07/23 15:12:26 WARN manager.MySQLManager: It looks like you are importing from mysql.
17/07/23 15:12:26 WARN manager.MySQLManager: This transfer can be faster! Use the --direct
17/07/23 15:12:26 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.
17/07/23 15:12:26 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)
17/07/23 15:12:26 ERROR tool.ImportTool: Import failed: No primary key could be found for table transactions. Please specify one with --split-by or perform a sequential import with '-m 1'.
-----------------------------------------------------------------------------------------------
sqoop import --connect jdbc:mysql://101.53.130.146/guptanayancy --username guptanayancy -P --table transactions --split-by id  --target-dir transactions_data
-----------------------------------------------------------------------------------------------------
guptanayancy@e2e-27-153:~$ sqoop import --connect jdbc:mysql://101.53.130.146/guptanayancy --username guptanayancy -P --table transactions --split-by id  --target-dir transactions_data
Warning: /opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
17/07/23 15:15:44 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.11.1
Enter password:
17/07/23 15:16:05 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
17/07/23 15:16:05 INFO tool.CodeGenTool: Beginning code generation
Sun Jul 23 15:16:05 IST 2017 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
17/07/23 15:16:05 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `transactions` AS t LIMIT 1
17/07/23 15:16:05 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `transactions` AS t LIMIT 1
17/07/23 15:16:05 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce
Note: /tmp/sqoop-guptanayancy/compile/5d0159b4def52411a1d8dbb5fd3418a6/transactions.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
17/07/23 15:16:07 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-guptanayancy/compile/5d0159b4def52411a1d8dbb5fd3418a6/transactions.jar
17/07/23 15:16:07 WARN manager.MySQLManager: It looks like you are importing from mysql.
17/07/23 15:16:07 WARN manager.MySQLManager: This transfer can be faster! Use the --direct
17/07/23 15:16:07 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.
17/07/23 15:16:07 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)
17/07/23 15:16:07 INFO mapreduce.ImportJobBase: Beginning import of transactions
17/07/23 15:16:07 INFO Configuration.deprecation: mapred.jar is deprecated. Instead, use mapreduce.job.jar
17/07/23 15:16:08 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps
17/07/23 15:16:08 INFO client.RMProxy: Connecting to ResourceManager at e2e-27-137.e2enetworks.net.in/101.53.130.137:8032
Sun Jul 23 15:16:10 IST 2017 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
17/07/23 15:16:10 INFO db.DBInputFormat: Using read commited transaction isolation
17/07/23 15:16:10 INFO db.DataDrivenDBInputFormat: BoundingValsQuery: SELECT MIN(`id`), MAX(`id`) FROM `transactions`
17/07/23 15:16:11 WARN db.TextSplitter: Generating splits for a textual index column.
17/07/23 15:16:11 WARN db.TextSplitter: If your database sorts in a case-insensitive order, this may result in a partial import or duplicate records.
17/07/23 15:16:11 WARN db.TextSplitter: You are strongly encouraged to choose an integral split column.
17/07/23 15:16:11 INFO mapreduce.JobSubmitter: number of splits:4
17/07/23 15:16:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1500551408671_2108
17/07/23 15:16:11 INFO impl.YarnClientImpl: Submitted application application_1500551408671_2108
17/07/23 15:16:11 INFO mapreduce.Job: The url to track the job: http://e2e-27-137.e2enetworks.net.in:19099/proxy/application_1500551408671_2108/
17/07/23 15:16:11 INFO mapreduce.Job: Running job: job_1500551408671_2108
17/07/23 15:16:20 INFO mapreduce.Job: Job job_1500551408671_2108 running in uber mode : false
17/07/23 15:16:20 INFO mapreduce.Job:  map 0% reduce 0%
17/07/23 15:16:27 INFO mapreduce.Job: Task Id : attempt_1500551408671_2108_m_000003_0, Status : FAILED
Error: org.apache.hadoop.hdfs.protocol.DSQuotaExceededException: The DiskSpace quota of /user/guptanayancy is exceeded: quota = 2147483648 B = 2 GB but diskspace consumed = 2207744060 B = 2.06 GB
        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyDiskspaceQuota(DirectoryWithQuotaFeature.java:149)
        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:159)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:2001)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1868)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1843)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(FSDirectory.java:441)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveAllocatedBlock(FSNamesystem.java:3806)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3394)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:683)
        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:214)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:495)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2220)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2216)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2214)

        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:106)
        at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:73)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1815)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1608)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:772)
Caused by: org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.protocol.DSQuotaExceededException): The DiskSpace quota of /user/guptanayancy is exceeded: quota = 2147483648 B = 2 GB but diskspace consumed = 2207744060 B = 2.06 GB
        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyDiskspaceQuota(DirectoryWithQuotaFeature.java:149)
        at org.apache.hadoop.hdfs.server.namenode.DirectoryWithQuotaFeature.verifyQuota(DirectoryWithQuotaFeature.java:159)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.verifyQuota(FSDirectory.java:2001)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1868)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.updateCount(FSDirectory.java:1843)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.addBlock(FSDirectory.java:441)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.saveAllocatedBlock(FSNamesystem.java:3806)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:3394)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:683)
        at org.apache.hadoop.hdfs.server.namenode.AuthorizationProviderProxyClientProtocol.addBlock(AuthorizationProviderProxyClientProtocol.java:214)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:495)
        at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:617)
        at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1073)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2220)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2216)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:415)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1920)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2214)

        at org.apache.hadoop.ipc.Client.call(Client.java:1502)
        at org.apache.hadoop.ipc.Client.call(Client.java:1439)
        at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:230)
        at com.sun.proxy.$Proxy17.addBlock(Unknown Source)
        at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:413)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:256)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:104)
        at com.sun.proxy.$Proxy18.addBlock(Unknown Source)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.locateFollowingBlock(DFSOutputStream.java:1812)
        ... 2 more

17/07/23 15:16:28 INFO mapreduce.Job:  map 50% reduce 0%
17/07/23 15:16:29 INFO mapreduce.Job:  map 75% reduce 0%
17/07/23 15:16:35 INFO mapreduce.Job:  map 100% reduce 0%
17/07/23 15:16:35 INFO mapreduce.Job: Job job_1500551408671_2108 completed successfully
17/07/23 15:16:35 INFO mapreduce.Job: Counters: 33
        File System Counters
                FILE: Number of bytes read=0
                FILE: Number of bytes written=615164
                FILE: Number of read operations=0
                FILE: Number of large read operations=0
                FILE: Number of write operations=0
                HDFS: Number of bytes read=529
                HDFS: Number of bytes written=60317773
                HDFS: Number of read operations=16
                HDFS: Number of large read operations=0
                HDFS: Number of write operations=8
        Job Counters
                Failed map tasks=1
                Launched map tasks=5
                Other local map tasks=5
                Total time spent by all maps in occupied slots (ms)=121976
                Total time spent by all reduces in occupied slots (ms)=0
                Total time spent by all map tasks (ms)=30494
                Total vcore-milliseconds taken by all map tasks=30494
                Total megabyte-milliseconds taken by all map tasks=62451712
        Map-Reduce Framework
                Map input records=1048575
                Map output records=1048575
                Input split bytes=529
                Spilled Records=0
                Failed Shuffles=0
                Merged Map outputs=0
                GC time elapsed (ms)=344
                CPU time spent (ms)=16810
                Physical memory (bytes) snapshot=1464127488
                Virtual memory (bytes) snapshot=6494216192
                Total committed heap usage (bytes)=2576351232
                Peak Map Physical memory (bytes)=451104768
                Peak Map Virtual memory (bytes)=1630801920
        File Input Format Counters
                Bytes Read=0
        File Output Format Counters
                Bytes Written=60317773
17/07/23 15:16:35 INFO mapreduce.ImportJobBase: Transferred 57.5235 MB in 27.3553 seconds (2.1028 MB/sec)
17/07/23 15:16:35 INFO mapreduce.ImportJobBase: Retrieved 1048575 records.
guptanayancy@e2e-27-153:~$ hadoop fs -ls /user/guptanayancy/
Found 9 items
drwx------   - guptanayancy labusers          0 2017-07-23 15:09 /user/guptanayancy/.Trash
drwxr-xr-x   - guptanayancy labusers          0 2017-07-22 16:56 /user/guptanayancy/.sparkStaging
drwx------   - guptanayancy labusers          0 2017-07-23 15:16 /user/guptanayancy/.staging
-rw-r--r--   3 guptanayancy labusers  111503503 2017-07-22 16:53 /user/guptanayancy/apache.access.log.PROJECT
drwxr-xr-x   - guptanayancy labusers          0 2017-07-22 18:55 /user/guptanayancy/case1
drwxr-xr-x   - guptanayancy labusers          0 2017-07-22 18:56 /user/guptanayancy/oozie-oozi
drwxr-xr-x   - guptanayancy labusers          0 2017-07-22 16:56 /user/guptanayancy/sparksql
-rw-r--r--   3 guptanayancy labusers         42 2017-07-15 18:33 /user/guptanayancy/sql.txt
drwxr-xr-x   - guptanayancy labusers          0 2017-07-23 15:16 /user/guptanayancy/transactions_data
guptanayancy@e2e-27-153:~$ hadoop fs -ls /user/guptanayancy/transactions_data
Found 5 items
-rw-r--r--   3 guptanayancy labusers          0 2017-07-23 15:16 /user/guptanayancy/transactions_data/_SUCCESS
-rw-r--r--   3 guptanayancy labusers    7880230 2017-07-23 15:16 /user/guptanayancy/transactions_data/part-m-00000
-rw-r--r--   3 guptanayancy labusers    6175607 2017-07-23 15:16 /user/guptanayancy/transactions_data/part-m-00001
-rw-r--r--   3 guptanayancy labusers   20992989 2017-07-23 15:16 /user/guptanayancy/transactions_data/part-m-00002
-rw-r--r--   3 guptanayancy labusers   25268947 2017-07-23 15:16 /user/guptanayancy/transactions_data/part-m-00003
guptanayancy@e2e-27-153:~$
--------------------------------------
guptanayancy@e2e-27-153:~$ sqoop import --connect jdbc:mysql://101.53.130.146/guptanayancy --username guptanayancy -password vwOT67OkP13Yocm --table transactions -m 2  --target-dir transactions_data
Warning: /opt/cloudera/parcels/CDH-5.11.1-1.cdh5.11.1.p0.4/bin/../lib/sqoop/../accumulo does not exist! Accumulo imports will fail.
Please set $ACCUMULO_HOME to the root of your Accumulo installation.
17/07/23 15:32:02 INFO sqoop.Sqoop: Running Sqoop version: 1.4.6-cdh5.11.1
17/07/23 15:32:02 WARN tool.BaseSqoopTool: Setting your password on the command-line is insecure. Consider using -P instead.
17/07/23 15:32:03 INFO manager.MySQLManager: Preparing to use a MySQL streaming resultset.
17/07/23 15:32:03 INFO tool.CodeGenTool: Beginning code generation
Sun Jul 23 15:32:03 IST 2017 WARN: Establishing SSL connection without server's identity verification is not recommended. According to MySQL 5.5.45+, 5.6.26+ and 5.7.6+ requirements SSL connection must be established by default if explicit option isn't set. For compliance with existing applications not using SSL the verifyServerCertificate property is set to 'false'. You need either to explicitly disable SSL by setting useSSL=false, or set useSSL=true and provide truststore for server certificate verification.
17/07/23 15:32:03 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `transactions` AS t LIMIT 1
17/07/23 15:32:03 INFO manager.SqlManager: Executing SQL statement: SELECT t.* FROM `transactions` AS t LIMIT 1
17/07/23 15:32:03 INFO orm.CompilationManager: HADOOP_MAPRED_HOME is /opt/cloudera/parcels/CDH/lib/hadoop-mapreduce
Note: /tmp/sqoop-guptanayancy/compile/5a16f536c13d127c2c5f13d2a033da48/transactions.java uses or overrides a deprecated API.
Note: Recompile with -Xlint:deprecation for details.
17/07/23 15:32:04 INFO orm.CompilationManager: Writing jar file: /tmp/sqoop-guptanayancy/compile/5a16f536c13d127c2c5f13d2a033da48/transactions.jar
17/07/23 15:32:04 WARN manager.MySQLManager: It looks like you are importing from mysql.
17/07/23 15:32:04 WARN manager.MySQLManager: This transfer can be faster! Use the --direct
17/07/23 15:32:04 WARN manager.MySQLManager: option to exercise a MySQL-specific fast path.
17/07/23 15:32:04 INFO manager.MySQLManager: Setting zero DATETIME behavior to convertToNull (mysql)
17/07/23 15:32:04 ERROR tool.ImportTool: Import failed: No primary key could be found for table transactions. Please specify one with --split-by or perform a sequential import with '-m 1'.
guptanayancy@e2e-27-153:~$

----------------------


transactions = LOAD 'transactions_data' USING PigStorage(',') as (id:chararray,chain:chararray,dept:chararray,category:chararray,
company:chararray,brand:chararray,date:chararray, productsize:float, productmeasure:chararray, purchasequantity:int, purchaseamount:float);

custGroup = GROUP transactions BY chain; 
custSpendings = FOREACH custGroup GENERATE group,SUM(transactions.purchasequantity) as quantity, SUM(transactions.purchaseamount) as spendings; 
custSpendingsSort = ORDER custSpendings BY spendings desc;
top10Cust = LIMIT custSpendingsSort 10;
DUMP top10Cust;
STORE top10Cust INTO ‘user/guptanayancy/liveoutput’ 


------------
transactions = LOAD 'transactions_data' USING PigStorage(',') as (id:chararray,chain:chararray,dept:chararray,category:chararray,
company:chararray,brand:chararray,date:chararray, productsize:float, productmeasure:chararray, purchasequantity:int, purchaseamount:float);

custGroup = GROUP transactions BY chain; 
custSpendings = FOREACH custGroup GENERATE group,SUM(transactions.purchasequantity) as quantity, SUM(transactions.purchaseamount) as spendings; 
DUMP custSpendings;
STORE top10Cust INTO ‘user/guptanayancy/liveoutput’ 




-------

tran1=group transactions by id;

tran2=foreach tran1 generate group,sum(transactions.purchaseamount) as spending;

tran3=order tran2n by spending desc;

tran4=limit tran3 10;

dump tran4;

store tran4 into 'tran4'

~


sqoop import --connect jdbc:mysql://101.53.130.146/guptanayancy --username guptanayancy --password vwOT67OkP13Yocm --table transactions --hive-import --hive-database guptanayancy --hive-table transactions_staging -m 1 













